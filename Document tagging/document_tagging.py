# -*- coding: utf-8 -*-
"""Document tagging

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e3KM4j2RKIM257a7tFeMvBFWGKjMuhzU
"""

# Installing the necessary files

!pip install transformers
!pip install simpletransformers

# Importing the files
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras.preprocessing.text import Tokenizer, text_to_word_sequence
import re, os
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import torch
import pickle

# Unzipping the data file ( Change the path of the file as per your requirements)
!unzip '/content/drive/MyDrive/Colab Datasets/Document Tagging.zip'

# Paths for the respective files
train_doc_path = '/content/Document Tagging/Train_docs'
train_tag_path = '/content/Document Tagging/Train_tags'
test_doc_path = '/content/Document Tagging/Test_docs'

# Reading all the training documnets into train_docs list

train_docs = []

for file in (os.listdir(train_doc_path)):
 train_docs.append(open(os.path.join(train_doc_path, file), mode = 'rb').read().lower())

# Encoding the documnets since it has been read in the bytes format
encoded_docs = []

for docs in train_docs:
  encoded_docs.append(docs.decode("latin-1"))

# Reading all the training tags into tag_docs list

tag_docs = []

for file in (os.listdir(train_tag_path)):
 tag_docs.append(open(os.path.join(train_tag_path, file), mode = 'rb').read().lower())

# Encoding the tags since it has been read in the bytes format
encoded_tags = []

for tags in tag_docs:
  encoded_tags.append(tags.decode("latin-1"))

# Reading all the test docs into test_docs list
test_docs = []

for test in (os.listdir(test_doc_path)):
 test_docs.append(open(os.path.join(test_doc_path, test), mode = 'rb').read().lower())

# Encoding the test docs since it has been read in the bytes format
encoded_test = []

for tst in test_docs:
  encoded_test.append(tst.decode("latin-1"))

# Creating a pandas dataframe with Docs and Tags as columns
train_df = pd.DataFrame(encoded_docs, encoded_tags).reset_index()

# Naming the columns as input_text and target_text in order to provide this df as input to the model.
# Renaming it some other column names causes error

train_df.columns = ['target_text', 'input_text']

test_df = pd.DataFrame(encoded_test)
test_df.columns = ['input_text']

from simpletransformers.seq2seq import Seq2SeqModel

# Separating 0.1% of training data as evaluation data
eval_df = train_df.sample(frac=0.1, random_state=42)

train_df = train_df.drop(eval_df.index)

# model parameters
model_args = {
    "reprocess_input_data": True,
    "overwrite_output_dir": True,
    "save_model_every_epoch": False,
    "save_eval_checkpoints": False,
    "max_seq_length": 512,
    "train_batch_size": 1,
    "num_train_epochs": 2,
}

# Check if GPU supports CUDA
cuda_available = torch.cuda.is_available()

# Create a Bart-base model
model = Seq2SeqModel(encoder_decoder_type="bart",
                    encoder_decoder_name="facebook/bart-base",
                    args=model_args, use_cuda = False)

# Empty the cache so that you don't run out of memory
torch.cuda.empty_cache()

# Train the maodel
model.train_model(train_df, batch_size = 1)

# Evaluate the model
result = model.eval_model(eval_df)
print(result)

# Saving the model for future use
pickle.dump(model, open('document_tagging.pkl', 'wb'))

# Testing the accuracy of predictions

test_df = eval_df

for idx, row in test_df.iterrows():

    plot = row['input_text']
    true_title = row['target_text']

    # Predict with trained BART model
    predicted_title = model.predict([plot])[0]

    print(f'True Title: {true_title}\n')
    print(f'Predicted Title: {predicted_title}\n')
    print(f'Plot: {plot}\n\n\n')

# Laoding the saved model

model = pickle.load(open('model_document_tagging.pkl', 'rb'))

# Passing the test_docs dataframe to generate the tags for it

predicted_title = []

for idx, row in test_df.iterrows():

    plot = row['input_text']

    # Predict with trained BART model
    title = model.predict([plot])[0]
    
    predicted_title.append(title)

# Creating a dataframe 

output_df = pd.DataFrame(list(zip(test_df['input_text'], predicted_title)))

output_df.columns = ['Doc', 'Predicted Title']

# Storing the final doc in csv format
output_df.to_csv('Predicted tags.csv')